<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial on LibriSpeech &mdash; Wenet  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tutorial on AIShell" href="tutorial_aishell.html" />
    <link rel="prev" title="Papers" href="papers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Wenet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Tutorial:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="papers.html">Papers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial on LibriSpeech</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#setup-environment">Setup environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#first-experiment">First Experiment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#stage-1-download-data">Stage -1: Download data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-0-prepare-training-data">Stage 0: Prepare Training data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-1-extract-optinal-cmvn-features">Stage 1: Extract optinal cmvn features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-2-generate-label-token-dictionary">Stage 2: Generate label token dictionary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-3-prepare-wenet-data-format">Stage 3: Prepare WeNet data format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-4-neural-network-training">Stage 4: Neural Network training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-5-recognize-wav-using-the-trained-model">Stage 5: Recognize wav using the trained model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-6-optional-export-the-trained-model">Stage 6(Optional): Export the trained model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-7-optional-add-lm-and-test-it-with-runtime">Stage 7(Optional): Add LM and test it with runtime</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_aishell.html">Tutorial on AIShell</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Pretrained Models in WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="lm.html">LM for WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="context.html">Context Biasing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Runtime for WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit_in_wenet.html">JIT in WeNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="UIO.html">UIO for WeNet</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Wenet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Tutorial on LibriSpeech</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tutorial_librispeech.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="tutorial-on-librispeech">
<h1>Tutorial on LibriSpeech<a class="headerlink" href="#tutorial-on-librispeech" title="Permalink to this headline"></a></h1>
<p>If you meet any problems when going through this tutorial, please feel free to ask in github <a class="reference external" href="https://github.com/mobvoi/wenet/issues">issues</a>. Thanks for any kind of feedback.</p>
<div class="section" id="setup-environment">
<h2>Setup environment<a class="headerlink" href="#setup-environment" title="Permalink to this headline"></a></h2>
<p>Please follow <a class="reference external" href="https://github.com/wenet-e2e/wenet#installation">Installation</a> to install WeNet.</p>
</div>
<div class="section" id="first-experiment">
<h2>First Experiment<a class="headerlink" href="#first-experiment" title="Permalink to this headline"></a></h2>
<p>We provide a recipe <code class="docutils literal notranslate"><span class="pre">example/librispeech/s0/run.sh</span></code> on librispeech data.</p>
<p>The recipe is simple and we suggest you run each stage one by one manually and check the result to understand the whole process.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">example</span><span class="o">/</span><span class="n">librispeech</span><span class="o">/</span><span class="n">s0</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="o">-</span><span class="mi">1</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">0</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">0</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">1</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">1</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">2</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">2</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">3</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">3</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">4</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">4</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">5</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">5</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">6</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">6</span>
<span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="mi">7</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">7</span>
</pre></div>
</div>
<p>You could also just run the whole script</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">run</span><span class="o">.</span><span class="n">sh</span> <span class="o">--</span><span class="n">stage</span> <span class="o">-</span><span class="mi">1</span> <span class="o">--</span><span class="n">stop</span><span class="o">-</span><span class="n">stage</span> <span class="mi">7</span>
</pre></div>
</div>
<div class="section" id="stage-1-download-data">
<h3>Stage -1: Download data<a class="headerlink" href="#stage-1-download-data" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">data_url</span><span class="o">=</span>www.openslr.org/resources/12
<span class="nv">datadir</span><span class="o">=</span>/export/data/en-asr-data/OpenSLR/
<span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le -1 <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge -1 <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="nb">echo</span> <span class="s2">&quot;stage -1: Data Download&quot;</span>
  <span class="k">for</span> part in dev-clean test-clean dev-other test-other train-clean-100 train-clean-360 train-other-500<span class="p">;</span> <span class="k">do</span>
    local/download_and_untar.sh <span class="si">${</span><span class="nv">datadir</span><span class="si">}</span> <span class="si">${</span><span class="nv">data_url</span><span class="si">}</span> <span class="si">${</span><span class="nv">part</span><span class="si">}</span>
  <span class="k">done</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>This stage downloads the librispeech data to the local path <code class="docutils literal notranslate"><span class="pre">$data</span></code>. This may take several hours. If you have already downloaded the data, please change the <code class="docutils literal notranslate"><span class="pre">$data</span></code> variable in <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> and start from <code class="docutils literal notranslate"><span class="pre">--stage</span> <span class="pre">0</span></code>.</p>
</div>
<div class="section" id="stage-0-prepare-training-data">
<h3>Stage 0: Prepare Training data<a class="headerlink" href="#stage-0-prepare-training-data" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">wave_data</span><span class="o">=</span>data
<span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">0</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="k">for</span> part in dev-clean test-clean dev-other test-other train-clean-100 train-clean-360 train-other-500<span class="p">;</span> <span class="k">do</span>
    <span class="c1"># use underscore-separated names in data directories.</span>
    local/data_prep_torchaudio.sh <span class="si">${</span><span class="nv">datadir</span><span class="si">}</span>/LibriSpeech/<span class="si">${</span><span class="nv">part</span><span class="si">}</span> <span class="nv">$wave_data</span>/<span class="si">${</span><span class="nv">part</span><span class="p">//-/_</span><span class="si">}</span>
  <span class="k">done</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>In this stage, <code class="docutils literal notranslate"><span class="pre">local/data_prep_torchaudio.sh</span></code> organizes the original data into two files:</p>
<ul class="simple">
<li><p><strong>wav.scp</strong> each line records two tab-separated columns : <code class="docutils literal notranslate"><span class="pre">wav_id</span></code> and <code class="docutils literal notranslate"><span class="pre">wav_path</span></code></p></li>
<li><p><strong>text</strong>  each line records two tab-separated columns :  <code class="docutils literal notranslate"><span class="pre">wav_id</span></code> and <code class="docutils literal notranslate"><span class="pre">text_label</span></code></p></li>
</ul>
<p><strong>wav.scp</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1867</span><span class="o">-</span><span class="mi">154075</span><span class="o">-</span><span class="mi">0014</span> <span class="o">/</span><span class="n">export</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">en</span><span class="o">-</span><span class="n">asr</span><span class="o">-</span><span class="n">data</span><span class="o">/</span><span class="n">OpenSLR</span><span class="o">//</span><span class="n">LibriSpeech</span><span class="o">/</span><span class="n">train</span><span class="o">-</span><span class="n">clean</span><span class="o">-</span><span class="mi">100</span><span class="o">/</span><span class="mi">1867</span><span class="o">/</span><span class="mi">154075</span><span class="o">/</span><span class="mi">1867</span><span class="o">-</span><span class="mi">154075</span><span class="o">-</span><span class="mf">0014.</span><span class="n">flac</span>
<span class="mi">1970</span><span class="o">-</span><span class="mi">26100</span><span class="o">-</span><span class="mi">0022</span> <span class="o">/</span><span class="n">export</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">en</span><span class="o">-</span><span class="n">asr</span><span class="o">-</span><span class="n">data</span><span class="o">/</span><span class="n">OpenSLR</span><span class="o">//</span><span class="n">LibriSpeech</span><span class="o">/</span><span class="n">train</span><span class="o">-</span><span class="n">clean</span><span class="o">-</span><span class="mi">100</span><span class="o">/</span><span class="mi">1970</span><span class="o">/</span><span class="mi">26100</span><span class="o">/</span><span class="mi">1970</span><span class="o">-</span><span class="mi">26100</span><span class="o">-</span><span class="mf">0022.</span><span class="n">flac</span>
<span class="o">...</span>
</pre></div>
</div>
<p><strong>text</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1867</span><span class="o">-</span><span class="mi">154075</span><span class="o">-</span><span class="mi">0014</span> <span class="n">YOU</span> <span class="n">SHOW</span> <span class="n">HIM</span> <span class="n">THAT</span> <span class="n">IT</span> <span class="n">IS</span> <span class="n">POSSIBLE</span>
<span class="mi">1970</span><span class="o">-</span><span class="mi">26100</span><span class="o">-</span><span class="mi">0022</span> <span class="n">DID</span> <span class="n">YOU</span> <span class="n">SEE</span> <span class="n">HIM</span> <span class="n">AT</span> <span class="n">THAT</span> <span class="n">TIME</span>
<span class="o">...</span>
</pre></div>
</div>
<p>If you want to train using your customized data, just organize the data into two files <code class="docutils literal notranslate"><span class="pre">wav.scp</span></code> and <code class="docutils literal notranslate"><span class="pre">text</span></code>, and start from <code class="docutils literal notranslate"><span class="pre">stage</span> <span class="pre">1</span></code>.</p>
</div>
<div class="section" id="stage-1-extract-optinal-cmvn-features">
<h3>Stage 1: Extract optinal cmvn features<a class="headerlink" href="#stage-1-extract-optinal-cmvn-features" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">1</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">1</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1">### Task dependent. You have to design training and dev sets by yourself.</span>
  <span class="c1">### But you can utilize Kaldi recipes in most cases</span>
  <span class="nb">echo</span> <span class="s2">&quot;stage 1: Feature Generation&quot;</span>
  mkdir -p <span class="nv">$wave_data</span>/train_960
  <span class="c1"># merge total training data</span>
  <span class="k">for</span> <span class="nb">set</span> in train_clean_100 train_clean_360 train_other_500<span class="p">;</span> <span class="k">do</span>
    <span class="k">for</span> f in <span class="sb">`</span>ls <span class="nv">$wave_data</span>/<span class="nv">$set</span><span class="sb">`</span><span class="p">;</span> <span class="k">do</span>
      cat <span class="nv">$wave_data</span>/<span class="nv">$set</span>/<span class="nv">$f</span> &gt;&gt; <span class="nv">$wave_data</span>/train_960/<span class="nv">$f</span>
    <span class="k">done</span>
  <span class="k">done</span>
  mkdir -p <span class="nv">$wave_data</span>/dev
  <span class="c1"># merge total dev data</span>
  <span class="k">for</span> <span class="nb">set</span> in dev_clean dev_other<span class="p">;</span> <span class="k">do</span>
    <span class="k">for</span> f in <span class="sb">`</span>ls <span class="nv">$wave_data</span>/<span class="nv">$set</span><span class="sb">`</span><span class="p">;</span> <span class="k">do</span>
      cat <span class="nv">$wave_data</span>/<span class="nv">$set</span>/<span class="nv">$f</span> &gt;&gt; <span class="nv">$wave_data</span>/dev/<span class="nv">$f</span>
    <span class="k">done</span>
  <span class="k">done</span>

  tools/compute_cmvn_stats.py --num_workers <span class="m">16</span> --train_config <span class="nv">$train_config</span> <span class="se">\</span>
    --in_scp <span class="nv">$wave_data</span>/<span class="nv">$train_set</span>/wav.scp <span class="se">\</span>
    --out_cmvn <span class="nv">$wave_data</span>/<span class="nv">$train_set</span>/global_cmvn

<span class="k">fi</span>
</pre></div>
</div>
<p>The librispeech corpus contains 3 subsets for training, namely <code class="docutils literal notranslate"><span class="pre">train_clean_100</span></code>, <code class="docutils literal notranslate"><span class="pre">train_clean_360</span></code>, and <code class="docutils literal notranslate"><span class="pre">train_other_500</span></code>,
so we first merge them to get our final training data.</p>
<p><code class="docutils literal notranslate"><span class="pre">tools/compute_cmvn_stats.py</span></code> is used to extract global cmvn(cepstral mean and variance normalization) statistics. These statistics will be used to normalize the acoustic features. Setting <code class="docutils literal notranslate"><span class="pre">cmvn=false</span></code> will skip this step.</p>
</div>
<div class="section" id="stage-2-generate-label-token-dictionary">
<h3>Stage 2: Generate label token dictionary<a class="headerlink" href="#stage-2-generate-label-token-dictionary" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">dict</span><span class="o">=</span><span class="nv">$wave_data</span>/lang_char/<span class="si">${</span><span class="nv">train_set</span><span class="si">}</span>_<span class="si">${</span><span class="nv">bpemode</span><span class="si">}${</span><span class="nv">nbpe</span><span class="si">}</span>_units.txt
<span class="nv">bpemodel</span><span class="o">=</span><span class="nv">$wave_data</span>/lang_char/<span class="si">${</span><span class="nv">train_set</span><span class="si">}</span>_<span class="si">${</span><span class="nv">bpemode</span><span class="si">}${</span><span class="nv">nbpe</span><span class="si">}</span>
<span class="nb">echo</span> <span class="s2">&quot;dictionary: </span><span class="si">${</span><span class="nv">dict</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">2</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">2</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1">### Task dependent. You have to check non-linguistic symbols used in the corpus.</span>
  <span class="nb">echo</span> <span class="s2">&quot;stage 2: Dictionary and Json Data Preparation&quot;</span>
  mkdir -p data/lang_char/

  <span class="nb">echo</span> <span class="s2">&quot;&lt;blank&gt; 0&quot;</span> &gt; <span class="si">${</span><span class="nv">dict</span><span class="si">}</span> <span class="c1"># 0 will be used for &quot;blank&quot; in CTC</span>
  <span class="nb">echo</span> <span class="s2">&quot;&lt;unk&gt; 1&quot;</span> &gt;&gt; <span class="si">${</span><span class="nv">dict</span><span class="si">}</span> <span class="c1"># &lt;unk&gt; must be 1</span>

  <span class="c1"># we borrowed these code and scripts which are related bpe from ESPnet.</span>
  cut -f <span class="m">2</span>- -d<span class="s2">&quot; &quot;</span> <span class="nv">$wave_data</span>/<span class="si">${</span><span class="nv">train_set</span><span class="si">}</span>/text &gt; <span class="nv">$wave_data</span>/lang_char/input.txt
  tools/spm_train --input<span class="o">=</span><span class="nv">$wave_data</span>/lang_char/input.txt --vocab_size<span class="o">=</span><span class="si">${</span><span class="nv">nbpe</span><span class="si">}</span> --model_type<span class="o">=</span><span class="si">${</span><span class="nv">bpemode</span><span class="si">}</span> --model_prefix<span class="o">=</span><span class="si">${</span><span class="nv">bpemodel</span><span class="si">}</span> --input_sentence_size<span class="o">=</span><span class="m">100000000</span>
  tools/spm_encode --model<span class="o">=</span><span class="si">${</span><span class="nv">bpemodel</span><span class="si">}</span>.model --output_format<span class="o">=</span>piece &lt; <span class="nv">$wave_data</span>/lang_char/input.txt <span class="p">|</span> tr <span class="s1">&#39; &#39;</span> <span class="s1">&#39;\n&#39;</span> <span class="p">|</span> sort <span class="p">|</span> uniq <span class="p">|</span> awk <span class="s1">&#39;{print $0 &quot; &quot; NR+1}&#39;</span> &gt;&gt; <span class="si">${</span><span class="nv">dict</span><span class="si">}</span>
  <span class="nv">num_token</span><span class="o">=</span><span class="k">$(</span>cat <span class="nv">$dict</span> <span class="p">|</span> wc -l<span class="k">)</span>
  <span class="nb">echo</span> <span class="s2">&quot;&lt;sos/eos&gt; </span><span class="nv">$num_token</span><span class="s2">&quot;</span> &gt;&gt; <span class="nv">$dict</span> <span class="c1"># &lt;eos&gt;</span>
  wc -l <span class="si">${</span><span class="nv">dict</span><span class="si">}</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>The model unit of English e2e speech recognition system could be char or BPE(byte-pair-encoding).
Typically, BPE shows better result. So here we use BPE as model unit,
and the BPE is trained by <a class="reference external" href="https://github.com/google/sentencepiece">sentencepiece</a> tool on the librispeech training data.</p>
<p>The model unit is defined as a dict in WeNet, which maps the a BPE into integer index.
The librispeech dict is like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;blank&gt; 0
&lt;unk&gt; 1
&#39; 2
▁ 3
A 4
▁A 5
AB 6
▁AB 7
▁YOU 4995
▁YOUNG 4996
▁YOUR 4997
▁YOUTH 4998
Z 4999
ZZ 5000
&lt;sos/eos&gt; 5001
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;blank&gt;</span></code> denotes the blank symbol for CTC.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;unk&gt;</span></code> denotes the unknown token, any out-of-vocabulary tokens will be mapped into it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;sos/eos&gt;</span></code> denotes start-of-speech and end-of-speech symbols for attention based encoder decoder training, and they shares the same id.</p></li>
</ul>
</div>
<div class="section" id="stage-3-prepare-wenet-data-format">
<h3>Stage 3: Prepare WeNet data format<a class="headerlink" href="#stage-3-prepare-wenet-data-format" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">3</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">3</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1"># Prepare wenet requried data</span>
  <span class="nb">echo</span> <span class="s2">&quot;Prepare data, prepare requried format&quot;</span>
  <span class="k">for</span> x in dev <span class="si">${</span><span class="nv">recog_set</span><span class="si">}</span> <span class="nv">$train_set</span> <span class="p">;</span> <span class="k">do</span>
    tools/make_raw_list.py <span class="nv">$wave_data</span>/<span class="nv">$x</span>/wav.scp <span class="nv">$wave_data</span>/<span class="nv">$x</span>/text <span class="se">\</span>
        <span class="nv">$wave_data</span>/<span class="nv">$x</span>/data.list
  <span class="k">done</span>

<span class="k">fi</span>
</pre></div>
</div>
<p>This stage generates the WeNet required format file <code class="docutils literal notranslate"><span class="pre">data.list</span></code>. Each line in <code class="docutils literal notranslate"><span class="pre">data.list</span></code> is in json format which contains the following fields.</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">key</span></code>: key of the utterance</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wav</span></code>: audio file path of the utterance</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">txt</span></code>: normalized transcription of the utterance, the transcription will be tokenized to the model units on-the-fly at the training stage.</p></li>
</ol>
<p>Here is an example of the <code class="docutils literal notranslate"><span class="pre">data.list</span></code>, and please see the generated training feature file in <code class="docutils literal notranslate"><span class="pre">data/train/data.list</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;1455-134435-0000&quot;</span><span class="p">,</span> <span class="s2">&quot;wav&quot;</span><span class="p">:</span> <span class="s2">&quot;/mnt/nfs/ptm1/open-data/LibriSpeech/train-clean-100/1455/134435/1455-134435-0000.flac&quot;</span><span class="p">,</span> <span class="s2">&quot;txt&quot;</span><span class="p">:</span> <span class="s2">&quot;THE GIRL WHO CAME INTO THE WORLD ON THAT NIGHT WHEN JESSE RAN THROUGH THE FIELDS CRYING TO GOD THAT HE BE GIVEN A SON HAD GROWN TO WOMANHOOD ON THE FARM&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;1455-134435-0001&quot;</span><span class="p">,</span> <span class="s2">&quot;wav&quot;</span><span class="p">:</span> <span class="s2">&quot;/mnt/nfs/ptm1/open-data/LibriSpeech/train-clean-100/1455/134435/1455-134435-0001.flac&quot;</span><span class="p">,</span> <span class="s2">&quot;txt&quot;</span><span class="p">:</span> <span class="s2">&quot;AND WHEN NOT ANGRY SHE WAS OFTEN MOROSE AND SILENT IN WINESBURG IT WAS SAID THAT SHE DRANK HER HUSBAND THE BANKER&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;1455-134435-0002&quot;</span><span class="p">,</span> <span class="s2">&quot;wav&quot;</span><span class="p">:</span> <span class="s2">&quot;/mnt/nfs/ptm1/open-data/LibriSpeech/train-clean-100/1455/134435/1455-134435-0002.flac&quot;</span><span class="p">,</span> <span class="s2">&quot;txt&quot;</span><span class="p">:</span> <span class="s2">&quot;BUT LOUISE COULD NOT BE MADE HAPPY SHE FLEW INTO HALF INSANE FITS OF TEMPER DURING WHICH SHE WAS SOMETIMES SILENT SOMETIMES NOISY AND QUARRELSOME SHE SWORE AND CRIED OUT IN HER ANGER SHE GOT A KNIFE FROM THE KITCHEN AND THREATENED HER HUSBAND&#39;S LIFE&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p>We aslo design another format for <code class="docutils literal notranslate"><span class="pre">data.list</span></code> named <code class="docutils literal notranslate"><span class="pre">shard</span></code> which is for big data training.
Please see <a class="reference external" href="https://github.com/wenet-e2e/wenet/tree/main/examples/gigaspeech/s0">gigaspeech</a>(10k hours) or
<a class="reference external" href="https://github.com/wenet-e2e/wenet/tree/main/examples/wenetspeech/s0">wenetspeech</a>(10k hours)
for how to use <code class="docutils literal notranslate"><span class="pre">shard</span></code> style <code class="docutils literal notranslate"><span class="pre">data.list</span></code> if you want to apply WeNet on big data set(more than 5k).</p>
</div>
<div class="section" id="stage-4-neural-network-training">
<h3>Stage 4: Neural Network training<a class="headerlink" href="#stage-4-neural-network-training" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">4</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">4</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1"># Training</span>
  mkdir -p <span class="nv">$dir</span>
  <span class="nv">INIT_FILE</span><span class="o">=</span><span class="nv">$dir</span>/ddp_init
  rm -f <span class="nv">$INIT_FILE</span> <span class="c1"># delete old one before starting</span>
  <span class="nv">init_method</span><span class="o">=</span>file://<span class="k">$(</span>readlink -f <span class="nv">$INIT_FILE</span><span class="k">)</span>
  <span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$0</span><span class="s2">: init method is </span><span class="nv">$init_method</span><span class="s2">&quot;</span>
  <span class="nv">num_gpus</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span> <span class="p">|</span> awk -F <span class="s2">&quot;,&quot;</span> <span class="s1">&#39;{print NF}&#39;</span><span class="k">)</span>
  <span class="c1"># Use &quot;nccl&quot; if it works, otherwise use &quot;gloo&quot;</span>
  <span class="nv">dist_backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span>
  <span class="nv">cmvn_opts</span><span class="o">=</span>
  <span class="nv">$cmvn</span> <span class="o">&amp;&amp;</span> <span class="nv">cmvn_opts</span><span class="o">=</span><span class="s2">&quot;--cmvn </span><span class="nv">$wave_data</span><span class="s2">/</span><span class="si">${</span><span class="nv">train_set</span><span class="si">}</span><span class="s2">/global_cmvn&quot;</span>
  <span class="c1"># train.py will write $train_config to $dir/train.yaml with model input</span>
  <span class="c1"># and output dimension, train.yaml will be used for inference or model</span>
  <span class="c1"># export later</span>
  <span class="k">for</span> <span class="o">((</span><span class="nv">i</span> <span class="o">=</span> <span class="m">0</span><span class="p">;</span> i &lt; <span class="nv">$num_gpus</span><span class="p">;</span> ++i<span class="o">))</span><span class="p">;</span> <span class="k">do</span>
  <span class="o">{</span>
    <span class="nv">gpu_id</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span> <span class="p">|</span> cut -d<span class="s1">&#39;,&#39;</span> -f$<span class="o">[</span><span class="nv">$i</span>+1<span class="o">]</span><span class="k">)</span>
    python wenet/bin/train.py --gpu <span class="nv">$gpu_id</span> <span class="se">\</span>
      --config <span class="nv">$train_config</span> <span class="se">\</span>
      --data_type raw <span class="se">\</span>
      --symbol_table <span class="nv">$dict</span> <span class="se">\</span>
      --train_data <span class="nv">$wave_data</span>/<span class="nv">$train_set</span>/data.list <span class="se">\</span>
      --cv_data <span class="nv">$wave_data</span>/dev/data.list <span class="se">\</span>
      <span class="si">${</span><span class="nv">checkpoint</span><span class="p">:+--checkpoint </span><span class="nv">$checkpoint</span><span class="si">}</span> <span class="se">\</span>
      --model_dir <span class="nv">$dir</span> <span class="se">\</span>
      --ddp.init_method <span class="nv">$init_method</span> <span class="se">\</span>
      --ddp.world_size <span class="nv">$num_gpus</span> <span class="se">\</span>
      --ddp.rank <span class="nv">$i</span> <span class="se">\</span>
      --ddp.dist_backend <span class="nv">$dist_backend</span> <span class="se">\</span>
      --num_workers <span class="m">1</span> <span class="se">\</span>
      <span class="nv">$cmvn_opts</span> <span class="se">\</span>
      --pin_memory
  <span class="o">}</span> <span class="p">&amp;</span>
  <span class="k">done</span>
  <span class="nb">wait</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>The NN model is trained in this step.</p>
<ul class="simple">
<li><p>Multi-GPU mode</p></li>
</ul>
<p>If using DDP mode for multi-GPU, we suggest using <code class="docutils literal notranslate"><span class="pre">dist_backend=&quot;nccl&quot;</span></code>. If the NCCL does not work, try using <code class="docutils literal notranslate"><span class="pre">gloo</span></code> or use <code class="docutils literal notranslate"><span class="pre">torch==1.6.0</span></code>
Set the GPU ids in CUDA_VISIBLE_DEVICES. For example, set <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">CUDA_VISIBLE_DEVICES=&quot;0,1,2,3,6,7&quot;</span></code> to use card 0,1,2,3,6,7.</p>
<ul class="simple">
<li><p>Resume training</p></li>
</ul>
<p>If your experiment is terminated after running several epochs for some reasons (e.g. the GPU is accidentally used by other people and is out-of-memory ), you could continue the training from a checkpoint model. Just find out the finished epoch in <code class="docutils literal notranslate"><span class="pre">exp/your_exp/</span></code>, set  <code class="docutils literal notranslate"><span class="pre">checkpoint=exp/your_exp/$n.pt</span></code> and run the <code class="docutils literal notranslate"><span class="pre">run.sh</span> <span class="pre">--stage</span> <span class="pre">4</span></code>. Then the training will continue from the $n+1.pt</p>
<ul class="simple">
<li><p>Config</p></li>
</ul>
<p>The config of neural network structure, optimization parameter, loss parameters, and dataset can be set in a YAML format file.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">conf/</span></code>,  we provide several models like transformer and conformer. see <code class="docutils literal notranslate"><span class="pre">conf/train_conformer.yaml</span></code> for reference.</p>
<ul class="simple">
<li><p>Use Tensorboard</p></li>
</ul>
<p>The training takes several hours. The actual time depends on the number and type of your GPU cards. In an 8-card 2080 Ti machine, it takes about less than one day for 50 epochs.
You could use tensorboard to monitor the loss.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir tensorboard/$your_exp_name/ --port 12598 --bind_all
</pre></div>
</div>
</div>
<div class="section" id="stage-5-recognize-wav-using-the-trained-model">
<h3>Stage 5: Recognize wav using the trained model<a class="headerlink" href="#stage-5-recognize-wav-using-the-trained-model" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">5</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">5</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
<span class="c1"># Test model, please specify the model you want to test by --checkpoint</span>
  <span class="nv">cmvn_opts</span><span class="o">=</span>
  <span class="nv">$cmvn</span> <span class="o">&amp;&amp;</span> <span class="nv">cmvn_opts</span><span class="o">=</span><span class="s2">&quot;--cmvn data/</span><span class="si">${</span><span class="nv">train_set</span><span class="si">}</span><span class="s2">/global_cmvn&quot;</span>
  <span class="c1"># TODO, Add model average here</span>
  mkdir -p <span class="nv">$dir</span>/test
  <span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">average_checkpoint</span><span class="si">}</span> <span class="o">==</span> <span class="nb">true</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nv">decode_checkpoint</span><span class="o">=</span><span class="nv">$dir</span>/avg_<span class="si">${</span><span class="nv">average_num</span><span class="si">}</span>.pt
    <span class="nb">echo</span> <span class="s2">&quot;do model average and final checkpoint is </span><span class="nv">$decode_checkpoint</span><span class="s2">&quot;</span>
    python wenet/bin/average_model.py <span class="se">\</span>
      --dst_model <span class="nv">$decode_checkpoint</span> <span class="se">\</span>
      --src_path <span class="nv">$dir</span>  <span class="se">\</span>
      --num <span class="si">${</span><span class="nv">average_num</span><span class="si">}</span> <span class="se">\</span>
      --val_best
  <span class="k">fi</span>
  <span class="c1"># Specify decoding_chunk_size if it&#39;s a unified dynamic chunk trained model</span>
  <span class="c1"># -1 for full chunk</span>
  <span class="nv">decoding_chunk_size</span><span class="o">=</span>
  <span class="nv">ctc_weight</span><span class="o">=</span><span class="m">0</span>.5
  <span class="c1"># Polling GPU id begin with index 0</span>
  <span class="nv">num_gpus</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span> <span class="p">|</span> awk -F <span class="s2">&quot;,&quot;</span> <span class="s1">&#39;{print NF}&#39;</span><span class="k">)</span>
  <span class="nv">idx</span><span class="o">=</span><span class="m">0</span>
  <span class="k">for</span> <span class="nb">test</span> in <span class="nv">$recog_set</span><span class="p">;</span> <span class="k">do</span>
    <span class="k">for</span> mode in <span class="si">${</span><span class="nv">decode_modes</span><span class="si">}</span><span class="p">;</span> <span class="k">do</span>
    <span class="o">{</span>
      <span class="o">{</span>
        <span class="nv">test_dir</span><span class="o">=</span><span class="nv">$dir</span>/<span class="si">${</span><span class="nv">test</span><span class="si">}</span>_<span class="si">${</span><span class="nv">mode</span><span class="si">}</span>
        mkdir -p <span class="nv">$test_dir</span>
        <span class="nv">gpu_id</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span> <span class="nv">$CUDA_VISIBLE_DEVICES</span> <span class="p">|</span> cut -d<span class="s1">&#39;,&#39;</span> -f$<span class="o">[</span><span class="nv">$idx</span>+1<span class="o">]</span><span class="k">)</span>
        python wenet/bin/recognize.py --gpu <span class="nv">$gpu_id</span> <span class="se">\</span>
          --mode <span class="nv">$mode</span> <span class="se">\</span>
          --config <span class="nv">$dir</span>/train.yaml <span class="se">\</span>
          --data_type raw <span class="se">\</span>
          --test_data <span class="nv">$wave_data</span>/<span class="nv">$test</span>/data.list <span class="se">\</span>
          --checkpoint <span class="nv">$decode_checkpoint</span> <span class="se">\</span>
          --beam_size <span class="m">10</span> <span class="se">\</span>
          --batch_size <span class="m">1</span> <span class="se">\</span>
          --penalty <span class="m">0</span>.0 <span class="se">\</span>
          --dict <span class="nv">$dict</span> <span class="se">\</span>
          --result_file <span class="nv">$test_dir</span>/text_bpe <span class="se">\</span>
          --ctc_weight <span class="nv">$ctc_weight</span> <span class="se">\</span>
          <span class="si">${</span><span class="nv">decoding_chunk_size</span><span class="p">:+--decoding_chunk_size </span><span class="nv">$decoding_chunk_size</span><span class="si">}</span>

        tools/spm_decode --model<span class="o">=</span><span class="si">${</span><span class="nv">bpemodel</span><span class="si">}</span>.model --input_format<span class="o">=</span>piece &lt; <span class="nv">$test_dir</span>/text_bpe <span class="p">|</span> sed -e <span class="s2">&quot;s/▁/ /g&quot;</span> &gt; <span class="nv">$test_dir</span>/text
        python tools/compute-wer.py --char<span class="o">=</span><span class="m">1</span> --v<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
          <span class="nv">$wave_data</span>/<span class="nv">$test</span>/text <span class="nv">$test_dir</span>/text &gt; <span class="nv">$test_dir</span>/wer
      <span class="o">}</span> <span class="p">&amp;</span>

      <span class="o">((</span><span class="nv">idx</span><span class="o">+=</span><span class="m">1</span><span class="o">))</span>
      <span class="k">if</span> <span class="o">[</span> <span class="nv">$idx</span> -eq <span class="nv">$num_gpus</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
        <span class="nv">idx</span><span class="o">=</span><span class="m">0</span>
      <span class="k">fi</span>
    <span class="o">}</span>
    <span class="k">done</span>
  <span class="k">done</span>
  <span class="nb">wait</span>

<span class="k">fi</span>
</pre></div>
</div>
<p>This stage shows how to recognize a set of wavs into texts. It also shows how to do the model averaging.</p>
<ul class="simple">
<li><p>Average model</p></li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">${average_checkpoint}</span></code> is set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the best <code class="docutils literal notranslate"><span class="pre">${average_num}</span></code> models on cross validation set will be averaged to generate a boosted model and used for recognition.</p>
<ul class="simple">
<li><p>Decoding</p></li>
</ul>
<p>Recognition is also called decoding or inference. The function of the NN will be applied on the input acoustic feature sequence to output a sequence of text.</p>
<p>Four decoding methods are provided in WeNet:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ctc_greedy_search</span></code> : encoder + CTC greedy search</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctc_prefix_beam_search</span></code> :  encoder + CTC prefix beam search</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attention</span></code> : encoder + attention-based decoder decoding</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attention_rescoring</span></code> : rescoring the ctc candidates from ctc prefix beam search with encoder output on attention-based decoder.</p></li>
</ul>
<p>In general, attention_rescoring is the best method. Please see <a class="reference external" href="https://arxiv.org/pdf/2012.05481.pdf">U2 paper</a> for the details of these algorithms.</p>
<p><code class="docutils literal notranslate"><span class="pre">--beam_size</span></code> is a tunable parameter, a large beam size may get better results but also cause higher computation cost.</p>
<p><code class="docutils literal notranslate"><span class="pre">--batch_size</span></code> can be greater than 1 for “ctc_greedy_search” and “attention” decoding mode, and must be 1 for “ctc_prefix_beam_search” and “attention_rescoring” decoding mode.</p>
<ul class="simple">
<li><p>WER evaluation</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">tools/compute-wer.py</span></code> will calculate the word (or char) error rate of the result.</p>
</div>
<div class="section" id="stage-6-optional-export-the-trained-model">
<h3>Stage 6(Optional): Export the trained model<a class="headerlink" href="#stage-6-optional-export-the-trained-model" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">6</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">6</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="c1"># Export the best model you want</span>
  python wenet/bin/export_jit.py <span class="se">\</span>
    --config <span class="nv">$dir</span>/train.yaml <span class="se">\</span>
    --checkpoint <span class="nv">$dir</span>/avg_<span class="si">${</span><span class="nv">average_num</span><span class="si">}</span>.pt <span class="se">\</span>
    --output_file <span class="nv">$dir</span>/final.zip
<span class="k">fi</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">wenet/bin/export_jit.py</span></code> will export the trained model using Libtorch.
The exported model files can be easily used for C++ inference in our runtime.
It is required if you want to integrate language model(LM), as shown in Stage 7.</p>
</div>
<div class="section" id="stage-7-optional-add-lm-and-test-it-with-runtime">
<h3>Stage 7(Optional): Add LM and test it with runtime<a class="headerlink" href="#stage-7-optional-add-lm-and-test-it-with-runtime" title="Permalink to this headline"></a></h3>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stage</span><span class="si">}</span> -le <span class="m">7</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="o">[</span> <span class="si">${</span><span class="nv">stop_stage</span><span class="si">}</span> -ge <span class="m">7</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
  <span class="nv">lm</span><span class="o">=</span>data/local/lm
  <span class="nv">lexicon</span><span class="o">=</span>data/local/dict/lexicon.txt
  mkdir -p <span class="nv">$lm</span>
  mkdir -p data/local/dict

  <span class="c1"># 7.1 Download &amp; format LM</span>
  <span class="nv">which_lm</span><span class="o">=</span><span class="m">3</span>-gram.pruned.1e-7.arpa.gz
  <span class="k">if</span> <span class="o">[</span> ! -e <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>/<span class="si">${</span><span class="nv">which_lm</span><span class="si">}</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    wget http://www.openslr.org/resources/11/<span class="si">${</span><span class="nv">which_lm</span><span class="si">}</span> -P <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>
  <span class="k">fi</span>
  <span class="nb">echo</span> <span class="s2">&quot;unzip lm(</span><span class="nv">$which_lm</span><span class="s2">)...&quot;</span>
  gunzip -k <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>/<span class="si">${</span><span class="nv">which_lm</span><span class="si">}</span> -c &gt; <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>/lm.arpa
  <span class="nb">echo</span> <span class="s2">&quot;Lm saved as </span><span class="si">${</span><span class="nv">lm</span><span class="si">}</span><span class="s2">/lm.arpa&quot;</span>

  <span class="c1"># 7.2 Prepare dict</span>
  <span class="nv">unit_file</span><span class="o">=</span><span class="nv">$dict</span>
  <span class="nv">bpemodel</span><span class="o">=</span><span class="nv">$bpemodel</span>
  <span class="c1"># use $dir/words.txt (unit_file) and $dir/train_960_unigram5000 (bpemodel)</span>
  <span class="c1"># if you download pretrained librispeech conformer model</span>
  cp <span class="nv">$unit_file</span> data/local/dict/units.txt
  <span class="k">if</span> <span class="o">[</span> ! -e <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>/librispeech-lexicon.txt <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    wget http://www.openslr.org/resources/11/librispeech-lexicon.txt -P <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>
  <span class="k">fi</span>
  <span class="nb">echo</span> <span class="s2">&quot;build lexicon...&quot;</span>
  tools/fst/prepare_dict.py <span class="nv">$unit_file</span> <span class="si">${</span><span class="nv">lm</span><span class="si">}</span>/librispeech-lexicon.txt <span class="se">\</span>
    <span class="nv">$lexicon</span> <span class="nv">$bpemodel</span>.model
  <span class="nb">echo</span> <span class="s2">&quot;lexicon saved as &#39;</span><span class="nv">$lexicon</span><span class="s2">&#39;&quot;</span>

  <span class="c1"># 7.3 Build decoding TLG</span>
  tools/fst/compile_lexicon_token_fst.sh <span class="se">\</span>
     data/local/dict data/local/tmp data/local/lang
  tools/fst/make_tlg.sh data/local/lm data/local/lang data/lang_test <span class="o">||</span> <span class="nb">exit</span> <span class="m">1</span><span class="p">;</span>

  <span class="c1"># 7.4 Decoding with runtime</span>
  <span class="nv">fst_dir</span><span class="o">=</span>data/lang_test
  <span class="k">for</span> <span class="nb">test</span> in <span class="si">${</span><span class="nv">recog_set</span><span class="si">}</span><span class="p">;</span> <span class="k">do</span>
    ./tools/decode.sh --nj <span class="m">6</span> <span class="se">\</span>
      --beam <span class="m">10</span>.0 --lattice_beam <span class="m">5</span> --max_active <span class="m">7000</span> --blank_skip_thresh <span class="m">0</span>.98 <span class="se">\</span>
      --ctc_weight <span class="m">0</span>.5 --rescoring_weight <span class="m">1</span>.0 --acoustic_scale <span class="m">1</span>.2 <span class="se">\</span>
      --fst_path <span class="nv">$fst_dir</span>/TLG.fst <span class="se">\</span>
      data/<span class="nv">$test</span>/wav.scp data/<span class="nv">$test</span>/text <span class="nv">$dir</span>/final.zip <span class="nv">$fst_dir</span>/words.txt <span class="se">\</span>
      <span class="nv">$dir</span>/lm_with_runtime_<span class="si">${</span><span class="nv">test</span><span class="si">}</span>
    tail <span class="nv">$dir</span>/lm_with_runtime_<span class="si">${</span><span class="nv">test</span><span class="si">}</span>/wer
  <span class="k">done</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>LM is  only supported in runtime, you have to build the runtime as shown in <a class="reference external" href="https://github.com/wenet-e2e/wenet#installation">Installation</a>,
and please refer <a class="reference external" href="https://wenet-e2e.github.io/wenet/lm.html">LM for WeNet</a> for the details of LM design.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="papers.html" class="btn btn-neutral float-left" title="Papers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tutorial_aishell.html" class="btn btn-neutral float-right" title="Tutorial on AIShell" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, wenet-team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>