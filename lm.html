<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LM for WeNet &mdash; wenet  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=b3ba4146"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Context Biasing" href="context.html" />
    <link rel="prev" title="Production Runtime" href="production.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            wenet
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="python_package.html">Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">How to train models?</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="production.html">Production Runtime</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">LM for WeNet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#motivation">Motivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#system-design">System Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implementation">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#results">Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-use">How to use?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="context.html">Context Biasing</a></li>
<li class="toctree-l2"><a class="reference internal" href="runtime.html">Runtime for WeNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit_in_wenet.html">JIT in WeNet</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">wenet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="production.html">Production Runtime</a></li>
      <li class="breadcrumb-item active">LM for WeNet</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lm.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lm-for-wenet">
<h1>LM for WeNet<a class="headerlink" href="#lm-for-wenet" title="Permalink to this heading"></a></h1>
<p>WeNet uses n-gram based statistical language model and the WFST framework to support the custom language model.
And LM is only supported in runtime of WeNet.</p>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Permalink to this heading"></a></h2>
<p>Why n-gram based LM? This may be the first question many people will ask.
Now that LM based on RNN and Transformer is in full swing, why does WeNet go backward?
The reason is simple, it is for productivity.
The n-gram-based language model has mature and complete training tools,
any amount of corpus can be trained, the training is very fast, the hotfix is easy,
and it has a wide range of mature applications in actual products.</p>
<p>Why WFST? It may be the second question many people will ask.
Since both industry and research have been working so hard to abandon traditional speech recognition,
especially the complex decoding technology. Why does WeNet back?
The reason is also very simple, it is for productivity.
WFST is a standard and powerful tool in traditional speech recognition.
And based on this solution, we have mature and complete bug fix solutions and product solutions,
such as that we can use the replace function in WFST for class-based personalization such as contact recognition.</p>
<p>Therefore, just like WeNet’s design goal “Production first and Production Ready”,
LM in WeNet also puts productivity as the first priority.
So it draws on many very productive tools and solutions accumulated in traditional speech recognition.
The difference to traditional speech recognition are:</p>
<ol class="simple">
<li><p>The training in WeNet is pure end to end.</p></li>
<li><p>As described below, LM is optional in decoding, you can choose whether to use LM according to your needs and application scenarios.</p></li>
</ol>
</section>
<section id="system-design">
<h2>System Design<a class="headerlink" href="#system-design" title="Permalink to this heading"></a></h2>
<p>The whole system is shown in the bellowing picture. There are two ways to generate N-best.</p>
<p><img alt="LM System Design" src="_images/lm_system.png" /></p>
<ol class="simple">
<li><p>Without LM, we use CTC prefix beam search to generate N-best.</p></li>
<li><p>With LM, we use CTC WFST search to generate N-best and CTC WFST search is the traditional WFST based decoder.</p></li>
</ol>
<p>There are two main parts of the CTC WFST based search.</p>
<p>The first is building the decoding graph, which is to compose the model unit T, the lexicon L and the language model G into one unified graph TLG. And in which:</p>
<ol class="simple">
<li><p>T is the model unit in E2E training. Typically it’s char in Chinese, char or BPE in English.</p></li>
<li><p>L is the lexicon, the lexicon is very simple. What we need to do is just split a word into its modeling unit sequence.
For example, the word “我们” is split into two chars “我 们”, and the word “APPLE” is split into five letters “A P P L E”.
We can see there is no phonemes and there is no need to design pronunciation on purpose.</p></li>
<li><p>G is the language model, namely compiling the n-gram to standard WFST representation.</p></li>
</ol>
<p>The second is the decoder, which is the same as the traditional decoder, which uses the standard Viterbi beam search algorithm in decoding.</p>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading"></a></h2>
<p>WeNet draws on the decoder and related tools in Kaldi to support LM and WFST based decoding.
For ease of using and keeping independence, we directly migrated the code related to decoding in Kaldi to <a class="reference external" href="https://github.com/wenet-e2e/wenet/tree/main/runtime/core/kaldi">this directory</a> in WeNet runtime.
And modify and organize according to the following principles:</p>
<ol class="simple">
<li><p>To minimize changes, the migrated code remains the same directory structure as the original.</p></li>
<li><p>We use GLOG to replace the log system in Kaldi.</p></li>
<li><p>We modify the code format to meet the lint requirements of the code style in WeNet.</p></li>
</ol>
<p>The core code is https://github.com/wenet-e2e/wenet/blob/main/runtime/core/decoder/ctc_wfst_beam_search.cc,
which wraps the LatticeFasterDecoder in Kaldi.
And we use blank frame skipping to speed up decoding.</p>
<p>In addition, WeNet also migrated related tools for building the decoding graph,
such as arpa2fst, fstdeterminizestar, fsttablecompose, fstminimizeencoded, and other tools.
So all the tools related to LM are built-in tools and can be used out of the box.</p>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this heading"></a></h2>
<p>We get consistent gain (3%~10%) on different datasets,
including aishell, aishell2, and librispeech,
please go to the corresponding example dataset for the details.</p>
</section>
<section id="how-to-use">
<h2>How to use?<a class="headerlink" href="#how-to-use" title="Permalink to this heading"></a></h2>
<p>Here is an example from aishell, which shows how to prepare the dictionary, how to train the LM,
how to build the graph, and how to decode with the runtime.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1"># 7.1 Prepare dict</span>
<span class="nv">unit_file</span><span class="o">=</span><span class="nv">$dict</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>data/local/dict
cp<span class="w"> </span><span class="nv">$unit_file</span><span class="w"> </span>data/local/dict/units.txt
tools/fst/prepare_dict.py<span class="w"> </span><span class="nv">$unit_file</span><span class="w"> </span><span class="si">${</span><span class="nv">data</span><span class="si">}</span>/resource_aishell/lexicon.txt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data/local/dict/lexicon.txt
<span class="c1"># 7.2 Train lm</span>
<span class="nv">lm</span><span class="o">=</span>data/local/lm
mkdir<span class="w"> </span>-p<span class="w"> </span><span class="nv">$lm</span>
tools/filter_scp.pl<span class="w"> </span>data/train/text<span class="w"> </span><span class="se">\</span>
<span class="w">     </span><span class="nv">$data</span>/data_aishell/transcript/aishell_transcript_v0.8.txt<span class="w"> </span>&gt;<span class="w"> </span><span class="nv">$lm</span>/text
local/aishell_train_lms.sh
<span class="c1"># 7.3 Build decoding TLG</span>
tools/fst/compile_lexicon_token_fst.sh<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data/local/dict<span class="w"> </span>data/local/tmp<span class="w"> </span>data/local/lang
tools/fst/make_tlg.sh<span class="w"> </span>data/local/lm<span class="w"> </span>data/local/lang<span class="w"> </span>data/lang_test<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span><span class="p">;</span>
<span class="c1"># 7.4 Decoding with runtime</span>
./tools/decode.sh<span class="w"> </span>--nj<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--beam<span class="w"> </span><span class="m">15</span>.0<span class="w"> </span>--lattice_beam<span class="w"> </span><span class="m">7</span>.5<span class="w"> </span>--max_active<span class="w"> </span><span class="m">7000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--blank_skip_thresh<span class="w"> </span><span class="m">0</span>.98<span class="w"> </span>--ctc_weight<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span>--rescoring_weight<span class="w"> </span><span class="m">1</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--fst_path<span class="w"> </span>data/lang_test/TLG.fst<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dict_path<span class="w"> </span>data/lang_test/words.txt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data/test/wav.scp<span class="w"> </span>data/test/text<span class="w"> </span><span class="nv">$dir</span>/final.zip<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data/lang_test/units.txt<span class="w"> </span><span class="nv">$dir</span>/lm_with_runtime
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="production.html" class="btn btn-neutral float-left" title="Production Runtime" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="context.html" class="btn btn-neutral float-right" title="Context Biasing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, wenet-team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>